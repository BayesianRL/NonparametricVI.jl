var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [NonparametricVI]\nOrder   = [:function, :type]","category":"page"},{"location":"api/#NonparametricVI.compute_metric-Tuple{KernelizedSteinDiscrepancy, ParticleContainer, Any}","page":"API","title":"NonparametricVI.compute_metric","text":"compute_metric(\n    metric::KernelizedSteinDiscrepancy,\n    pc::ParticleContainer,\n    ρ;\n    ad_backend\n)\n\nCompute the Kernelized Stein Discrepancy (KSD) between the particles in the ParticleContainer and the target log-density.\n\nArguments\n\nmetric::KernelizedSteinDiscrepancy: A KernelizedSteinDiscrepancy object that defines the kernel and other parameters used for KSD computation.\npc::ParticleContainer: The particle container holding the current set of particles.\nρ: A LogDensityProblem representing the target distribution's log-density function.\n\nKeyword Arguments\n\nad_backend: The automatic differentiation backend to use for computing gradients required by the KSD.\n\nReturns\n\nThe computed Kernelized Stein Discrepancy (KSD) value, a scalar representing the discrepancy between the particle distribution and the target distribution.\n\nDetails\n\nThis function calculates the KSD, a measure of the discrepancy between the distribution represented by the particles in pc and the target distribution defined by ρ. It utilizes the kernel specified in the metric object and the provided automatic differentiation backend ad_backend to compute the necessary gradients.\n\nThe function extracts the particle positions from pc.P and calls the kernelized_stein_discrepancy function (which is assumed to be defined elsewhere) to perform the KSD computation.\n\nThis function serves as a metric tracking tool during particle-based inference, allowing monitoring of the convergence of the particle distribution to the target.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.constrained_particles-Tuple{ParticleContainer, DynamicPPL.Model}","page":"API","title":"NonparametricVI.constrained_particles","text":"constrained_particles(pc::ParticleContainer, model::DynamicPPL.Model)\n\nTranforming the position of samples in pc back to the constrained space\n\nArguments\n\npc::ParticleContainer: The container holding the unconstrained positions of the particles.\nmodel::DynamicPPL.Model: The target Turing program\n\nReturns\n\nsamples::Vector{<:DynamicPPL.VarInfo}: A vector of VarInfo objects, where each VarInfo contains a sample from the constrained parameter space of the model, corresponding to a particle in the ParticleContainer.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.get_samples-Tuple{ParticleContainer, NonparametricVI.SVGDInferenceState}","page":"API","title":"NonparametricVI.get_samples","text":"get_samples(pc::ParticleContainer, state::SVGDInferenceState)\n\nExtract constrained samples from the particle container for Turing models\n\nArguments\n\npc::ParticleContainer: The particle container holding the current set of particles.\nstate::SVGDInferenceState: The internal state for the SVGD algorithm\n\nReturns\n\nVector{DynamicPPL.VarInfo} a vector of varinfo containing parameters in the constrained space\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.infer!-Tuple{ParticleContainer, NonparametricVI.SVGDInferenceState}","page":"API","title":"NonparametricVI.infer!","text":"infer!(\n    pc::ParticleContainer,\n    state::SVGDInferenceState;\n    iters::Integer,\n    ad_backend=ADTypes.AutoForwardDiff(),\n    verbose::Bool=false,\n    track::Dict{String, Metric}=Dict()\n)\n\nPerform inference using Stein Variational Gradient Descent (SVGD).\n\nArguments\n\npc::ParticleContainer: The particle container holding the current set of particles. \nstate::SVGDInferenceState: The internal state object for the SVGD algorithm\n\nKeyword Arguments\n\niters::Integer: The number of SVGD iterations to perform.\nad_backend=ADTypes.AutoForwardDiff(): The automatic differentiation backend to use for computing gradients. Defaults to ADTypes.AutoForwardDiff().\nverbose::Bool=false: A boolean flag indicating whether to print progress information during the inference. Defaults to false.\n\nReturns\n\nSVGDInferenceReport: An SVGDInferenceReport object \n\nDetails\n\nThis function modifies the pc in-place, updating the positions of the particles.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.init-Tuple{Any, NonparametricVI.ParticleDynamics}","page":"API","title":"NonparametricVI.init","text":"init(\n    ρ,\n    dynamics::ParticleDynamics;\n    particle_initializer=NormalInitializer(),\n    n_particles::Integer,\n    ad_backend=ADTypes.AutoForwardDiff()\n)\n\nInitialize the particle container and the internal state for a particle-based inference algorithm, given a log-density problem.\n\nArguments\n\nρ: A LogDensityProblem representing the target distribution's log-density function.\ndynamics::ParticleDynamics: The particle dynamics object that governs how particles evolve.\n\nKeyword Arguments\n\nparticle_initializer: An object that initializes the positions of the particles. Defaults to NormalInitializer().\nn_particles::Integer: The number of particles to initialize. \nad_backend=ADTypes.AutoForwardDiff(): The automatic differentiation backend to use if the provided ρ is not differentiable. Defaults to ADTypes.AutoForwardDiff().\n\nReturns\n\npc: The initialized particle container, holding the initial positions of all particles. \nstate: The initialized internal state associated with the provided dynamics and ρ. \n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.init-Tuple{DynamicPPL.Model, NonparametricVI.ParticleDynamics}","page":"API","title":"NonparametricVI.init","text":"init(\n    mode::DynamicPPL.Model,\n    dynamics::ParticleDynamics;\n    particle_initializer=NormalInitializer(),\n    n_particles::Integer,\n    ad_backend=ADTypes.AutoForwardDiff()\n)\n\nInitialize the particle container and the internal state for a particle-based inference algorithm, given a log-density problem.\n\nArguments\n\nρ: A LogDensityProblem representing the target distribution's log-density function.\ndynamics::ParticleDynamics: The particle dynamics object that governs how particles evolve.\n\nKeyword Arguments\n\nparticle_initializer: An object that initializes the positions of the particles. Defaults to NormalInitializer().\nn_particles::Integer: The number of particles to initialize. \nad_backend=ADTypes.AutoForwardDiff(): The automatic differentiation backend to use if the provided ρ is not differentiable. Defaults to ADTypes.AutoForwardDiff().\n\nReturns\n\npc: The initialized particle container, holding the initial positions of all particles. \nstate: The initialized internal state associated with the provided dynamics and ρ. \n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.init_state-Tuple{Any, SVGD, DynamicPPL.Model}","page":"API","title":"NonparametricVI.init_state","text":"init_state(ρ, dynamics::SVGD, model::DynamicPPL.Model)\n\nInitialize the SVGDInferenceState for using SVGD dynamics on Turing models \n\nArguments\n\nρ: A LogDensityProblem representing the target distribution.\ndynamics::SVGD: An SVGD object defining the dynamics used to update the particles.\nmodel::DynamicPPL.Model: The probabilistic model associated with the inference.\n\nReturns\n\nAn SVGDInferenceState object initialized with the provided arguments.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.init_state-Tuple{Any, SVGD}","page":"API","title":"NonparametricVI.init_state","text":"init_state(ρ, dynamics::SVGD)\n\nInitialize the SVGDInferenceState for using SVGD dynamics with LogDensityProblem ρ\n\nArguments\n\nρ: A LogDensityProblem representing the target distribution's log-density function.\ndynamics::SVGD: An SVGD object defining the dynamics used to update the particles.\n\nReturns\n\nAn SVGDInferenceState object initialized with the provided arguments.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.kernel_and_gradient_fn-Tuple{KernelFunctions.Kernel, Any}","page":"API","title":"NonparametricVI.kernel_and_gradient_fn","text":"kernel_and_gradient_fn(K::KernelFunctions.Kernel, ad_backend)\n\nReturns a function that computes the kernel value and its gradient with respect to the first argument.\n\nArguments\n\nK::KernelFunctions.Kernel: The kernel function from KernelFunctions.jl\nad_backend: The automatic differentiation backend to use (e.g., AbstractDifferentiation.ForwardDiffBackend()).\n\nReturns\n\nA function k_∇k(x, a) that takes two arguments x and a (of compatible types for the kernel K) and returns a tuple containing:\nThe kernel value K(x, a).\nThe gradient of the kernel with respect to x, evaluated at x.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.kernelized_stein_discrepancy-Tuple{Any, Any, KernelFunctions.Kernel}","page":"API","title":"NonparametricVI.kernelized_stein_discrepancy","text":"kernelized_stein_discrepancy(P, q, K::KernelFunctions.Kernel; ad_backend)\n\nComputes the Kernelized Stein Discrepancy (KSD) between a set of samples P and a distribution q.\n\nThe KSD measures the discrepancy between two probability distributions by evaluating the expectation of two Stein operators applied to a kernel function.\n\nArguments\n\nP: A matrix of samples from the empirical distribution. Each column represents a sample.\nq: A LogDensityProblems.LogDensityProblem representing the target distribution.\nK: A kernel function from KernelFunctions.Kernel.\nad_backend: An automatic differentiation backend from DifferentiationInterface.\n\nReturns\n\nThe Kernelized Stein Discrepancy (KSD) as a scalar value.\n\nDetails\n\nThe function calculates the KSD using the following formula:\n\ntextKSD(P q) = frac1n(n-1) sum_i=1^n sum_j=1^n u(P_i P_j)\n\nu(x y) = nabla s(x)^T k(x y) nabla s(y) + nabla s(x)^T nabla_y k(x y) + nabla_x k(x y)^T nabla s(y) + texttr(nabla_xy k(x y))\n\nFor more details see :  \n\nA Kernelized Stein Discrepancy for Goodness-of-fit Tests, Qiang Liu, Jason Lee, Michael Jordan\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.logdensityproblem_from_turing-Tuple{DynamicPPL.Model, ADTypes.AbstractADType}","page":"API","title":"NonparametricVI.logdensityproblem_from_turing","text":"logdensityproblem_from_turing(model::DynamicPPL.Model, ad_backend::ADTypes.AbstractADType)\n\nConstructs a differentiable LogDensityProblem from a Turing DynamicPPL.Model\n\nThis function takes a Turing model and an automatic differentiation backend,  performs necessary transformations to the model's parameter space, and returns a LogDensityProblem that can be used with optimization or sampling algorithms that require gradient information.\n\nArguments\n\nmodel::DynamicPPL.Model: The Turing probabilistic model.\nad_backend::ADTypes.AbstractADType: The automatic differentiation backend to use (e.g., ADTypes.AutoForwardDiff(), ADTypes.AutoZygote()).\n\nReturns\n\n∇ρ::LogDensityProblemsAD.ADgradient: A LogDensityProblem object that wraps the log-density function of the Turing model and provides gradient computation capabilities using the specified ad_backend. The parameters of this problem are in the unconstrained space.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.particle_velocity-Tuple{ParticleContainer, Any, Any, Any, SVGD}","page":"API","title":"NonparametricVI.particle_velocity","text":"particle_velocity(pc::ParticleContainer, ρ, pi, k_∇k, dynamics::SVGD)\n\nComputes the velocity of a single particle based on the Stein Variational Gradient Descent (SVGD) update rule, potentially using a mini-batch of other particles.\n\nArguments\n\npc::ParticleContainer: The container holding the particles.\nρ: The log-density function (a LogDensityProblem) which must be differentiable.\npi::Int: The index of the particle for which to compute the velocity.\nk_∇k: A function that takes two particle positions (as vectors) and returns a tuple containing the kernel value and the gradient of the kernel with respect to the first argument. This is generated by kernel_and_gradient_fn.\ndynamics::SVGD: The SVGD dynamics object containing the kernel, step size, and batch size.\n\nReturns\n\nvelocity::Vector: The computed velocity vector for the particle with index pi.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.update_particles!-Tuple{Any, ParticleContainer, SVGD, Any}","page":"API","title":"NonparametricVI.update_particles!","text":"update_particles!(ρ, pc::ParticleContainer, dynamics::SVGD)\n\nUpdates the positions of all particles in the ParticleContainer according to the Stein Variational Gradient Descent (SVGD) update rule.\n\nArguments\n\nρ: The log-density function (a LogDensityProblem) that the particles aim to sample from.\npc::ParticleContainer: The container holding the current positions of the particles. The particle positions are updated in-place.\ndynamics::SVGD: The SVGD dynamics object specifying the kernel, step size (η), and batch size for the update.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.KernelizedSteinDiscrepancy","page":"API","title":"NonparametricVI.KernelizedSteinDiscrepancy","text":"KernelizedSteinDiscrepancy <: Metric\n\nA struct representing the Kernelized Stein Discrepancy (KSD) metric.\n\nFields\n\nK::KernelFunctions.Kernel: The kernel function used in the KSD computation.\n\nDescription\n\nThis struct encapsulates the kernel function needed to compute the Kernelized Stein Discrepancy. The KSD is a measure of discrepancy between two probability distributions, and it relies on a kernel function to define the feature space in which the discrepancy is measured.\n\nBy holding the kernel function, this struct provides a convenient way to pass the necessary information for KSD computation to functions that track or evaluate the discrepancy between a particle distribution and a target distribution.\n\n\n\n\n\n","category":"type"},{"location":"api/#NonparametricVI.SVGD","page":"API","title":"NonparametricVI.SVGD","text":"SVGD <: ParticleDynamics\n\nStein Variational Gradient Descent (SVGD) particle dynamics.\n\nFields\n\nK::KernelFunctions.Kernel: The kernel function used to define the interaction between particles.\nη: The step size or learning rate for updating particle positions.\nbatchsize: The number of particles to use in each update step (for mini-batching). If nothing, all particles are used.\n\nExamples\n\nusing KernelFunctions\n\nDefine a squared exponential kernel\n\nsqexp_kernel = SqExponentialKernel()\n\nCreate an SVGD dynamics object with a fixed step size and full batch\n\nsvgd_fullbatch = SVGD(K=sqexp_kernel, η=0.1, batchsize=nothing)\n\nCreate an SVGD dynamics object with a smaller step size and a batch size of 100\n\nsvgd_minibatch = SVGD(K=sqexp_kernel, η=0.05, batchsize=100)\n\n\n\n\n\n","category":"type"},{"location":"api/#NonparametricVI.SVGDInferenceReport","page":"API","title":"NonparametricVI.SVGDInferenceReport","text":"SVGDInferenceReport <: InferenceReport\n\nA mutable struct representing the report generated after running Stein Variational Gradient Descent (SVGD) inference.\n\nFields\n\nmetrics::Dict{String, Vector{Any}}: A dictionary storing various metrics tracked during the SVGD inference. Keys are metric names (strings), and values are vectors of metric values recorded at each iteration or relevant time step.\nsuccess::Bool: A boolean flag indicating whether the inference process completed successfully.\n\n\n\n\n\n","category":"type"},{"location":"api/#NonparametricVI.SVGDInferenceState","page":"API","title":"NonparametricVI.SVGDInferenceState","text":"SVGDInferenceState <: InferenceState\n\nA mutable struct representing the internal state of the Stein Variational Gradient Descent (SVGD) inference algorithm.\n\nFields\n\nρ: A LogDensityProblem representing the target distribution's log-density function.\ndynamics::SVGD: An SVGD object defining the dynamics used to update the particles.\nmodel::Union{DynamicPPL.Model, Nothing}: The probabilistic model associated with the inference, or nothing if no model is provided.\n\nDescription\n\nThis struct encapsulates the state information required by the SVGD algorithm. It stores the target log-density, the SVGD dynamics, and optionally, a reference to the DynamicPPL model used. This state is passed between iterations of the SVGD algorithm to maintain and update the necessary information.\n\n\n\n\n\n","category":"type"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"NonparametricVI.jl is under development, you can install the latest version using Pkg:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Pkg.add(url=\"https://github.com/BayesianRL/NonparametricVI.jl.git\")","category":"page"},{"location":"getting_started/#Using-with-Turing.jl-Probabilistic-Programs","page":"Getting Started","title":"Using with Turing.jl Probabilistic Programs","text":"","category":"section"},{"location":"getting_started/#Example:-Linear-Regression","page":"Getting Started","title":"Example: Linear Regression","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Let's craft a toy regression problem:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using NonparametricVI\nusing Turing\nusing LinearAlgebra\nusing KernelFunctions\nusing CairoMakie\n\nn = 100\nX = 2rand(n) .- 1.0\ny = 3X .+ 1 + randn(n)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The generated problem looks like this:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<p align=\"center\">\n    <img src=\"../assets/examples/linear_regression/data.png\" width=\"400\">\n</p>","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We start by defining a simple Turing.jl model for regression and instantiate it:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"@model function bayesian_regression(X, y)\n    α ~ Normal(0.0, 1.0)\n    β ~ Normal(0.0, 1.0)\n\n    for i in eachindex(y)\n        y[i] ~ Normal(α * X[i] + β, 0.5)\n    end\nend\n\nmodel = bayesian_regression(X, y)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To define the dynamics of Stein Variational Gradient Descent (SVGD), we need a positive-definite kernel. You can use all provided by KernelFunctions.jl. We use a scaled squared exponential kernel. for more details on designing more complex kernels, check out KernelFunctions.jl documentation:  ","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using KernelFunctions\nkernel = SqExponentialKernel() ∘ ScaleTransform(0.3)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Next we define the parameters of SVGD:  ","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"dynamics = SVGD(K=kernel, η=0.003, batchsize=32)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Nonparametric Variational Inference methods use a set of particles instead of a parametric family of distribution to approximate posterior (or any target) distribution. The init method creates the particles pc, in addition to an internal state state which will be used by the inference procedure.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"pc, state = init(model, dynamics; n_particles=128)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"pc is a simple struct containing position of particles. Using get_samples we can access the particles and plot them:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"samples = get_samples(pc, state)\nα_samples = [s[@varname(α)] for s in samples]\nβ_samples = [s[@varname(β)] for s in samples];","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Note that some Turing models may contain constrained parameters (e.g. positive, bounded, ...) while most inference methods are performed on an unconstrained space obtained by transforming the original denisty of parameters. The get_samples method transforms the particle positions back to the contrained space. Before running SVGD we can visualize the currest state of particles:  ","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<p align=\"center\">\n    <img src=\"../assets/examples/linear_regression/particles_before_inference.png\" width=\"400\">\n</p>","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Finally we can perform inference. Note the infer! method modifies the particles in-place.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"infer!(pc, state; iters=200)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"After collecting samples with get_samples we can visualize the final result:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<p align=\"center\">\n    <img src=\"../assets/examples/linear_regression/particles_after_inference.png\" width=\"400\">\n</p>","category":"page"},{"location":"getting_started/#Using-with-LogDensityProblems","page":"Getting Started","title":"Using with LogDensityProblems","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"In addtion to Turing programs, you can use NonparametricVI for a custom Bayesian inference problem by implementing the LogDensityProblems.jl interface. For example here we define a toy unnormalized mixture density:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"struct MixtureDensity end\n\nfunction LogDensityProblems.capabilities(::Type{<:MixtureDensity})\n    LogDensityProblems.LogDensityOrder{0}()\nend\n\nLogDensityProblems.dimension(::MixtureDensity) = 2\n\nfunction LogDensityProblems.logdensity(::MixtureDensity, x)\n    log(0.25 * exp(-1/0.5 * norm(x-[-1.5, -1.5])^2) +\n        0.25 * exp(-1/0.5 * norm(x-[-1.5,  1.5])^2) +\n        0.25 * exp(-1/0.5 * norm(x-[ 1.5, -1.5])^2) +\n        0.25 * exp(-1/0.5 * norm(x-[ 1.5,  1.5])^2))\nend\n\nρ = MixtureDensity()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Next we define the inference dynamics by choosing a custom kernel. It can be any kernel provided by KernelFunctions.jl. Here we use a scaled version of the squared exponential kernel:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"kernel = SqExponentialKernel() ∘ ScaleTransform(2.0)\ndynamics = SVGD(K=kernel, η=0.5, batchsize=16)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Now we create a set of particles that represent samples:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"pc, state = init(ρ, dynamics; n_particles=512)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can access particle positions by get_samples and visualize the their current position:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"S = get_samples(pc)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<p align=\"center\">\n    <img src=\"../assets/examples/mixture/particles_before_inference.png\" width=\"512\">\n</p>","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Obviously the initial samples does not match the target density. Now we run the SVGD dynamics to adjust the samples:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"infer!(pc, state; iters=100)\nS = get_samples(pc)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Finally we can check the terminal position of particles:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<p align=\"center\">\n    <img src=\"../assets/examples/mixture/particles_after_inference.png\" width=\"512\">\n</p>","category":"page"},{"location":"","page":"Homepage","title":"Homepage","text":"<p align=\"center\">\n    <img src=\"./assets/logo-temp.svg\" width=\"100%\">\n</p>","category":"page"}]
}

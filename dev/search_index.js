var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [NonparametricVI]\nOrder   = [:function, :type]","category":"page"},{"location":"api/#NonparametricVI.compute_metric-Tuple{KernelizedSteinDiscrepancy, ParticleContainer, Any}","page":"API","title":"NonparametricVI.compute_metric","text":"compute_metric(\n    metric::KernelizedSteinDiscrepancy,\n    pc::ParticleContainer,\n    ρ;\n    ad_backend::ADTypes.AbstractADType\n)\n\nCompute the Kernelized Stein Discrepancy (KSD) between the particles in the ParticleContainer and the target log-density.\n\nArguments\n\nmetric::KernelizedSteinDiscrepancy: A KernelizedSteinDiscrepancy object that defines the kernel and other parameters used for KSD computation.\npc::ParticleContainer: The particle container holding the current set of particles.\nρ: A LogDensityProblem representing the target distribution's log-density function.\n\nKeyword Arguments\n\nad_backend: The automatic differentiation backend to use for computing gradients required by the KSD.\n\nReturns\n\nThe computed Kernelized Stein Discrepancy (KSD) value, a scalar representing the discrepancy between the particle distribution and the target distribution.\n\nDetails\n\nThis function calculates the KSD, a measure of the discrepancy between the distribution represented by the particles in pc and the target distribution defined by ρ. It utilizes the kernel specified in the metric object and the provided automatic differentiation backend ad_backend to compute the necessary gradients.\n\nThe function extracts the particle positions from pc.P and calls the kernelized_stein_discrepancy function to perform the KSD computation.\n\nThis function serves as a metric tracking tool during particle-based inference, allowing monitoring of the convergence of the particle distribution to the target.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.get_problem-Tuple{NonparametricVI.LogDensityProblemContext}","page":"API","title":"NonparametricVI.get_problem","text":"get_problem(ctx::LogDensityProblemContext)\n\nReturns LogDensityProblem stored in LogDensityProblemContext.\n\nArguments\n\nctx::LogDensityProblemContext: The problem context\n\nReturns\n\nthe instance of LogDensityProblem ctx.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.infer!-Tuple{ParticleContainer, NonparametricVI.Context{<:NonparametricVI.AbstractProblemContext, NonparametricVI.SVGDInferenceContext}}","page":"API","title":"NonparametricVI.infer!","text":"infer!(\n    pc::ParticleContainer,\n    ctx::Context{<:AbstractProblemContext, SVGDInferenceContext};\n    iters::Integer=10,\n    ad_backend::ADTypes.AbstractADType=ADTypes.AutoForwardDiff(),\n    verbose::Bool=false,\n    track=Dict{String, Any}()\n)\n\nPerforms Stein Variational Gradient Descent (SVGD) inference to update a particle container.\n\nArguments\n\npc::ParticleContainer: The container holding the particles to be updated.\nctx::Context{<:AbstractProblemContext, SVGDInferenceContext}: The context containing the problem definition and the SVGD inference settings.\niters::Integer=10: The number of SVGD iterations to perform.\nad_backend::ADTypes.AbstractADType=ADTypes.AutoForwardDiff(): The automatic differentiation backend to use for gradient computations.\nverbose::Bool=false: A flag to enable verbose output during inference (currently not implemented).\ntrack::Dict{String, Any}(): A dictionary specifying metrics to compute and track during inference. The keys are metric names (strings), and the values are metric types (e.g., functions or structs that can be passed to compute_metric).\n\nReturns\n\nAn SVGDInferenceReport containing the tracked metrics and a success flag.\n\nNotes\n\nThis function retrieves the log-density function from the problem context.\nIt initializes and updates the particles using the SVGD dynamics specified in the inference context.\nOptionally, it computes and tracks specified metrics at each iteration.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.init-Tuple{Any, NonparametricVI.ParticleDynamics}","page":"API","title":"NonparametricVI.init","text":"init(\n    ρ,\n    dynamics::ParticleDynamics;\n    particle_initializer=NormalInitializer(),\n    n_particles::Integer=16,\n    ad_backend::ADTypes.AbstractADType=ADTypes.AutoForwardDiff()\n)\n\nInitializes a particle container and the corresponding inference context.\n\nArguments\n\nρ: The target log-density function.\ndynamics::ParticleDynamics: The particle dynamics to be used for inference.\nparticle_initializer: An object that initializes the particle positions (default: NormalInitializer()).\nn_particles::Integer=16: The number of particles to initialize (default: 16).\nad_backend::ADTypes.AbstractADType=ADTypes.AutoForwardDiff(): The automatic differentiation backend to use for gradient computations if needed (default: ADTypes.AutoForwardDiff()).\n\nReturns\n\nA tuple containing:\npc::ParticleContainer: The initialized particle container.\nctx::Context: The initialized inference context.\n\nNotes\n\nThis function determines the dimensionality of the problem from the log-density function ρ.\nIt ensures that the log-density function is differentiable by wrapping it with an AD backend.\nIt initializes the particle positions using the provided particle_initializer.\nIt initializes the inference context based on the provided dynamics.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.init_context-Tuple{Any, NonparametricVI.ParticleDynamics}","page":"API","title":"NonparametricVI.init_context","text":"init_context(\n    ρ,\n    dynamics::ParticleDynamics\n)\n\nInitializes a Context struct containing a problem context and an inference context.\n\nArguments\n\nρ: The target log-density function.\ndynamics::ParticleDynamics: The particle dynamics to be used for inference.\n\nReturns\n\nAn instance of Context containing:\nA problem context initialized from ρ.\nAn inference context initialized from ρ and dynamics.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.init_inference_context-Tuple{Any, SVGD}","page":"API","title":"NonparametricVI.init_inference_context","text":"init_inference_context(ρ, dynamics::SVGD)\n\nInitializes an SVGDInferenceContext with the provided SVGD dynamics. For now ρ is not used.\n\nArguments\n\nρ: The target LogDensityProblem\ndynamics::SVGD: The SVGD dynamics to be used for inference.\n\nReturns\n\nAn instance of SVGDInferenceContext initialized with the given dynamics.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.kernel_and_gradient_fn-Tuple{KernelFunctions.Kernel, ADTypes.AbstractADType}","page":"API","title":"NonparametricVI.kernel_and_gradient_fn","text":"kernel_and_gradient_fn(K::KernelFunctions.Kernel, ad_backend)\n\nReturns a function that computes the kernel value and its gradient with respect to the first argument.\n\nArguments\n\nK::KernelFunctions.Kernel: The kernel function from KernelFunctions.jl\nad_backend::ADTypes.AbstractADType: The automatic differentiation backend to use (e.g., AbstractDifferentiation.ForwardDiffBackend()).\n\nReturns\n\nA function k_∇k(x, a) that takes two arguments x and a (of compatible types for the kernel K) and returns a tuple containing:\nThe kernel value K(x, a).\nThe gradient of the kernel with respect to x, evaluated at x.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.kernelized_stein_discrepancy-Tuple{Any, Any, KernelFunctions.Kernel}","page":"API","title":"NonparametricVI.kernelized_stein_discrepancy","text":"kernelized_stein_discrepancy(P, q, K::KernelFunctions.Kernel; samplesize::Integer, ad_backend::ADTypes.AbstractADType)\n\nComputes the Kernelized Stein Discrepancy (KSD) between a set of samples P and a distribution q.\n\nThe KSD measures the discrepancy between two probability distributions by evaluating the expectation of two Stein operators applied to a kernel function.\n\nArguments\n\nP: A matrix of samples from the empirical distribution. Each column represents a sample.\nq: A LogDensityProblems.LogDensityProblem representing the target distribution.\nK: A kernel function from KernelFunctions.Kernel.\nsamplesize::Integer: The number of sampled particles to evaluate KSD\nad_backend::ADTypes.AbstractADType: An automatic differentiation backend from DifferentiationInterface.\n\nReturns\n\nThe Kernelized Stein Discrepancy (KSD) as a scalar value.\n\nDetails\n\nThe function calculates the KSD using the following formula:\n\ntextKSD(P q) = frac1n(n-1) sum_i=1^n sum_j=1^n u(P_i P_j)\n\nu(x y) = nabla s(x)^T k(x y) nabla s(y) + nabla s(x)^T nabla_y k(x y) + nabla_x k(x y)^T nabla s(y) + texttr(nabla_xy k(x y))\n\nFor more details see :  \n\nA Kernelized Stein Discrepancy for Goodness-of-fit Tests, Qiang Liu, Jason Lee, Michael Jordan\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.particle_velocity-Tuple{ParticleContainer, Any, Any, Any, SVGD}","page":"API","title":"NonparametricVI.particle_velocity","text":"particle_velocity(pc::ParticleContainer, ρ, pi, k_∇k, dynamics::SVGD)\n\nComputes the velocity of a single particle based on the Stein Variational Gradient Descent (SVGD) update rule, potentially using a mini-batch of other particles.\n\nArguments\n\npc::ParticleContainer: The container holding the particles.\nρ: The log-density function (a LogDensityProblem) which must be differentiable.\npi::Int: The index of the particle for which to compute the velocity.\nk_∇k: A function that takes two particle positions (as vectors) and returns a tuple containing the kernel value and the gradient of the kernel with respect to the first argument. This is generated by kernel_and_gradient_fn.\ndynamics::SVGD: The SVGD dynamics object containing the kernel, step size, and batch size.\n\nReturns\n\nvelocity::Vector: The computed velocity vector for the particle with index pi.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.update_particles!-Tuple{Any, ParticleContainer, SVGD, Any}","page":"API","title":"NonparametricVI.update_particles!","text":"update_particles!(ρ, pc::ParticleContainer, dynamics::SVGD)\n\nUpdates the positions of all particles in the ParticleContainer according to the Stein Variational Gradient Descent (SVGD) update rule.\n\nArguments\n\nρ: The log-density function (a LogDensityProblem) that the particles aim to sample from.\npc::ParticleContainer: The container holding the current positions of the particles. The particle positions are updated in-place.\ndynamics::SVGD: The SVGD dynamics object specifying the kernel, step size (η), and batch size for the update.\n\n\n\n\n\n","category":"method"},{"location":"api/#NonparametricVI.Context","page":"API","title":"NonparametricVI.Context","text":"Context{T<:AbstractProblemContext, U<:AbstractInferenceContext}\n\nA mutable struct that holds instances of a problem context and an inference context.\n\nFields\n\nproblem::T: An instance of a subtype of AbstractProblemContext.\ninference::U: An instance of a subtype of AbstractInferenceContext.\n\n\n\n\n\n","category":"type"},{"location":"api/#NonparametricVI.KernelizedSteinDiscrepancy","page":"API","title":"NonparametricVI.KernelizedSteinDiscrepancy","text":"KernelizedSteinDiscrepancy <: Metric\n\nA struct representing the Kernelized Stein Discrepancy (KSD) metric.\n\nFields\n\nK::KernelFunctions.Kernel: The kernel function used in the KSD computation.\nsamplesize::Integer: the size of sample used to evaluate KSD\n\nDescription\n\nThis struct encapsulates the kernel function needed to compute the Kernelized Stein Discrepancy. The KSD is a measure of discrepancy between two probability distributions, and it relies on a kernel function to define the feature space in which the discrepancy is measured.\n\nBy holding the kernel function, this struct provides a convenient way to pass the necessary information for KSD computation to functions that track or evaluate the discrepancy between a particle distribution and a target distribution.\n\n\n\n\n\n","category":"type"},{"location":"api/#NonparametricVI.LogDensityProblemContext","page":"API","title":"NonparametricVI.LogDensityProblemContext","text":"mutable struct LogDensityProblemContext <: AbstractProblemContext\n    ρ\nend\n\nA mutable struct representing the problem context for LogDensityProblem.jl\n\nFields\n\nρ: should implement LogDensityProblem interface\n\n\n\n\n\n","category":"type"},{"location":"api/#NonparametricVI.ParticleContainer","page":"API","title":"NonparametricVI.ParticleContainer","text":"mutable struct ParticleContainer{T} <: AbstractParticleContainer{T}\n    P::Matrix{T}\n    dim\n    size\nend\n\nA mutable struct that holds a collection of particles.\n\nFields\n\nP::Matrix{T}: A matrix where each column represents a particle.\ndim: The dimensionality of each particle (number of rows in P).\nsize: The number of particles (number of columns in P).\n\n\n\n\n\n","category":"type"},{"location":"api/#NonparametricVI.SVGD","page":"API","title":"NonparametricVI.SVGD","text":"SVGD <: ParticleDynamics\n\nStein Variational Gradient Descent (SVGD) particle dynamics.\n\nFields\n\nK::KernelFunctions.Kernel: The kernel function used to define the interaction between particles.\nη: The step size or learning rate for updating particle positions.\nbatchsize: The number of particles to use in each update step (for mini-batching). If nothing, all particles are used.\n\nExamples\n\nusing KernelFunctions\n\nDefine a squared exponential kernel\n\nsqexp_kernel = SqExponentialKernel()\n\nCreate an SVGD dynamics object with a fixed step size and full batch\n\nsvgd_fullbatch = SVGD(K=sqexp_kernel, η=0.1, batchsize=nothing)\n\nCreate an SVGD dynamics object with a smaller step size and a batch size of 100\n\nsvgd_minibatch = SVGD(K=sqexp_kernel, η=0.05, batchsize=100)\n\n\n\n\n\n","category":"type"},{"location":"api/#NonparametricVI.SVGDInferenceContext","page":"API","title":"NonparametricVI.SVGDInferenceContext","text":"struct SVGDInferenceContext <: AbstractInferenceContext\n    dynamics::SVGD\nend\n\nA struct representing the inference context for SVGD.\n\nFields\n\ndynamics::SVGD: An instance of the SVGD dynamics\n\n\n\n\n\n","category":"type"},{"location":"api/#NonparametricVI.SVGDInferenceReport","page":"API","title":"NonparametricVI.SVGDInferenceReport","text":"SVGDInferenceReport <: InferenceReport\n\nA mutable struct representing the report generated after running Stein Variational Gradient Descent (SVGD) inference.\n\nFields\n\nmetrics::Dict{String, Vector{Any}}: A dictionary storing various metrics tracked during the SVGD inference. Keys are metric names (strings), and values are vectors of metric values recorded at each iteration or relevant time step.\nsuccess::Bool: A boolean flag indicating whether the inference process completed successfully.\n\n\n\n\n\n","category":"type"},{"location":"getting_started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"","category":"section"},{"location":"getting_started/#Installation","page":"Getting Started","title":"Installation","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"NonparametricVI.jl is under development, you can install the latest version using Pkg:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Pkg.add(url=\"https://github.com/BayesianRL/NonparametricVI.jl.git\")","category":"page"},{"location":"getting_started/#Using-with-Turing.jl-Probabilistic-Programs","page":"Getting Started","title":"Using with Turing.jl Probabilistic Programs","text":"","category":"section"},{"location":"getting_started/#Example:-Linear-Regression","page":"Getting Started","title":"Example: Linear Regression","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Let's craft a toy regression problem:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using NonparametricVI\nusing Turing\nusing LinearAlgebra\nusing KernelFunctions\nusing CairoMakie\n\nn = 100\nX = 2rand(n) .- 1.0\ny = 3X .+ 1 + randn(n)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"The generated problem looks like this:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<p align=\"center\">\n    <img src=\"../assets/examples/linear_regression/data.png\" width=\"400\">\n</p>","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We start by defining a simple Turing.jl model for regression and instantiate it:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"@model function bayesian_regression(X, y)\n    α ~ Normal(0.0, 1.0)\n    β ~ Normal(0.0, 1.0)\n\n    for i in eachindex(y)\n        y[i] ~ Normal(α * X[i] + β, 0.5)\n    end\nend\n\nmodel = bayesian_regression(X, y)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"To define the dynamics of Stein Variational Gradient Descent (SVGD), we need a positive-definite kernel. You can use all provided by KernelFunctions.jl. We use a scaled squared exponential kernel. for more details on designing more complex kernels, check out KernelFunctions.jl documentation:  ","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"using KernelFunctions\nkernel = SqExponentialKernel() ∘ ScaleTransform(0.3)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Next we define the parameters of SVGD:  ","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"dynamics = SVGD(K=kernel, η=0.003, batchsize=32)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Nonparametric Variational Inference methods use a set of particles instead of a parametric family of distribution to approximate posterior (or any target) distribution. The init method creates the particles pc, in addition to an internal state state which will be used by the inference procedure.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"pc, state = init(model, dynamics; n_particles=128)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"pc is a simple struct containing position of particles. Using get_samples we can access the particles and plot them:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"samples = get_samples(pc, state)\nα_samples = [s[@varname(α)] for s in samples]\nβ_samples = [s[@varname(β)] for s in samples];","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Note that some Turing models may contain constrained parameters (e.g. positive, bounded, ...) while most inference methods are performed on an unconstrained space obtained by transforming the original denisty of parameters. The get_samples method transforms the particle positions back to the contrained space. Before running SVGD we can visualize the currest state of particles:  ","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<p align=\"center\">\n    <img src=\"../assets/examples/linear_regression/particles_before_inference.png\" width=\"400\">\n</p>","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Finally we can perform inference. Note the infer! method modifies the particles in-place.","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"infer!(pc, state; iters=200)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"After collecting samples with get_samples we can visualize the final result:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<p align=\"center\">\n    <img src=\"../assets/examples/linear_regression/particles_after_inference.png\" width=\"400\">\n</p>","category":"page"},{"location":"getting_started/#Using-with-LogDensityProblems","page":"Getting Started","title":"Using with LogDensityProblems","text":"","category":"section"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"In addtion to Turing programs, you can use NonparametricVI for a custom Bayesian inference problem by implementing the LogDensityProblems.jl interface. For example here we define a toy unnormalized mixture density:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"struct MixtureDensity end\n\nfunction LogDensityProblems.capabilities(::Type{<:MixtureDensity})\n    LogDensityProblems.LogDensityOrder{0}()\nend\n\nLogDensityProblems.dimension(::MixtureDensity) = 2\n\nfunction LogDensityProblems.logdensity(::MixtureDensity, x)\n    log(0.25 * exp(-1/0.5 * norm(x-[-1.5, -1.5])^2) +\n        0.25 * exp(-1/0.5 * norm(x-[-1.5,  1.5])^2) +\n        0.25 * exp(-1/0.5 * norm(x-[ 1.5, -1.5])^2) +\n        0.25 * exp(-1/0.5 * norm(x-[ 1.5,  1.5])^2))\nend\n\nρ = MixtureDensity()","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Next we define the inference dynamics by choosing a custom kernel. It can be any kernel provided by KernelFunctions.jl. Here we use a scaled version of the squared exponential kernel:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"kernel = SqExponentialKernel() ∘ ScaleTransform(2.0)\ndynamics = SVGD(K=kernel, η=0.5, batchsize=16)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Now we create a set of particles that represent samples:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"pc, state = init(ρ, dynamics; n_particles=512)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"We can access particle positions by get_samples and visualize the their current position:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"S = get_samples(pc)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<p align=\"center\">\n    <img src=\"../assets/examples/mixture/particles_before_inference.png\" width=\"512\">\n</p>","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Obviously the initial samples does not match the target density. Now we run the SVGD dynamics to adjust the samples:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"infer!(pc, state; iters=100)\nS = get_samples(pc)","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"Finally we can check the terminal position of particles:","category":"page"},{"location":"getting_started/","page":"Getting Started","title":"Getting Started","text":"<p align=\"center\">\n    <img src=\"../assets/examples/mixture/particles_after_inference.png\" width=\"512\">\n</p>","category":"page"},{"location":"","page":"Homepage","title":"Homepage","text":"<p align=\"center\">\n    <img src=\"./assets/logo-temp.svg\" width=\"100%\">\n</p>","category":"page"}]
}
